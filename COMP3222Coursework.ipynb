{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pccom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pccom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "F:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'onların', 'printr', 'sekurang', 'setidak', 'tama', 'tidaknya', 'δι', 'арбаң', 'арсалаң', 'афташ', 'бай', 'бале', 'баски', 'батыр', 'баҳри', 'болои', 'бүгжең', 'бұтыр', 'валекин', 'вақте', 'вой', 'вуҷуди', 'гар', 'гарчанде', 'далаң', 'даме', 'ербелең', 'жалт', 'жұлт', 'карда', 'кошки', 'куя', 'күңгір', 'кӣ', 'магар', 'майлаш', 'митың', 'модоме', 'нияти', 'онан', 'оре', 'паһ', 'рӯи', 'салаң', 'сар', 'сұлаң', 'сұрт', 'тарбаң', 'тразе', 'ту', 'тыржың', 'тұрс', 'хом', 'хуб', 'чаро', 'чи', 'чун', 'чунон', 'шарте', 'шұңқ', 'ыржың', 'қадар', 'қайқаң', 'қалт', 'қаңғыр', 'қаңқ', 'қош', 'қызараң', 'құйқаң', 'құлт', 'құңқ', 'ұрс', 'ҳай', 'ҳамин', 'ҳатто', 'ҳо', 'ҳол', 'ҳолате', 'әттеген', 'ӯим', 'अक', 'अग', 'अझ', 'अन', 'अर', 'आजक', 'आत', 'आद', 'आफ', 'आय', 'ईक', 'उद', 'उनक', 'उनल', 'उह', 'एउट', 'एन', 'कog', 'कत', 'कम', 'कस', 'कसर', 'कह', 'गत', 'गय', 'गर', 'चम', 'छन', 'जत', 'जबक', 'जस', 'जसक', 'जसब', 'जसम', 'जसल', 'जह', 'तत', 'तथ', 'तदन', 'तप', 'तवम', 'नज', 'नत', 'नभन', 'नय', 'पक', 'पछ', 'पन', 'पय', 'पर', 'पष', 'पह', 'बन', 'बर', 'भएक', 'भय', 'भव', 'मल', 'यत', 'यथ', 'यद', 'यप', 'यसक', 'यसपछ', 'यसब', 'यसर', 'यह', 'रण', 'रत', 'रमश', 'रह', 'लस', 'वर', 'सक', 'सट', 'सध', 'सपछ', 'सब', 'सम', 'सर', 'सह', 'हन', 'हर', 'हरण', 'ἀλλ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[2179  367]\n",
      " [ 311  898]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.88      0.86      0.87      2546\n",
      "        real       0.71      0.74      0.73      1209\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.79      0.80      0.80      3755\n",
      "weighted avg       0.82      0.82      0.82      3755\n",
      "\n",
      "Accuracy: 0.8194407456724367\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Random Forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#SVM classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "trainingPath = #insert path to the training set\n",
    "testPath = #insert path to the test set\n",
    "\n",
    "trainingSet = pd.read_csv(trainingPath, sep=\"\\t\")\n",
    "testSet = pd.read_csv(testPath, sep=\"\\t\")\n",
    "\n",
    "trainingDocs = []\n",
    "testDocs = []\n",
    " \n",
    "#\"_values\" refers to the \"tweetText\" column, and \"_labels\" to the \"label\" column\n",
    "trainingSet_values = trainingSet.iloc[:, 1].values\n",
    "trainingSet_labels = trainingSet.iloc[:, 6].values\n",
    "\n",
    "testSet_values = testSet.iloc[:, 1].values\n",
    "testSet_labels = testSet.iloc[:, 6].values    \n",
    "\n",
    "#We treat 'humor' labels as 'fake'\n",
    "for x in range(0, len(trainingSet_labels)):\n",
    "    if (trainingSet_labels[x]=='humor'):\n",
    "        trainingSet_labels[x] = 'fake'\n",
    "\n",
    "        \n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "#Pre-processing data in both sets\n",
    "for i in range(0, len(trainingSet)):\n",
    "    trainingSet_values = trainingSet.iloc[:, 1].values\n",
    "    \n",
    "    #Turning everything to lowercase\n",
    "    trainingDoc = trainingSet_values[i].lower()\n",
    "    #Removing special characters\n",
    "    trainingDoc = re.sub(r'\\W', ' ', str(trainingSet_values[i]))\n",
    "    #Turning multiple spaces into a single space\n",
    "    trainingDoc = re.sub(r'\\s+', ' ', trainingDoc, flags=re.I)\n",
    "    #Removing all non-Latin characters\n",
    "    #trainingDoc = re.sub(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]', u'', trainingDoc) \n",
    "    \n",
    "    #Performing lemmatisation\n",
    "    trainingDoc = trainingDoc.split()\n",
    "    trainingDoc = [stemmer.lemmatize(word) for word in trainingDoc]\n",
    "    trainingDoc = ' '.join(trainingDoc)\n",
    "    \n",
    "    trainingDocs.append(trainingDoc)\n",
    "\n",
    "for i in range(0, len(testSet)):\n",
    "    testSet_values = testSet.iloc[:, 1].values\n",
    "    \n",
    "    #Turning everything to lowercase\n",
    "    testDoc = testSet_values[i].lower()\n",
    "    #Removing special characters\n",
    "    testDoc = re.sub(r'\\W', ' ', str(testSet_values[i]))\n",
    "    #Turning multiple spaces into a single space\n",
    "    testDoc = re.sub(r'\\s+', ' ', testDoc, flags=re.I)\n",
    "    #Removing all non-Latin characters\n",
    "    testDoc = re.sub(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]', u'', testDoc) \n",
    "\n",
    "    #Performing lemmatisation\n",
    "    testDoc = testDoc.split()\n",
    "    testDoc = [stemmer.lemmatize(word) for word in testDoc]\n",
    "    testDoc = ' '.join(testDoc)\n",
    "    \n",
    "    testDocs.append(testDoc)\n",
    "\n",
    "    \n",
    "#Vectoriser for the BoW model\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=1, max_df=0.8, stop_words=stopwords.words())\n",
    "\n",
    "training = vectorizer.fit_transform(trainingDocs).toarray()\n",
    "test = vectorizer.fit_transform(testDocs).toarray()\n",
    "\n",
    "#TF-IDF transformer \n",
    "tfidfconverter = TfidfTransformer()#use_idf=False\n",
    "\n",
    "training = tfidfconverter.fit_transform(training).toarray()\n",
    "test = tfidfconverter.fit_transform(test).toarray()\n",
    "\n",
    "#Training and running the Random Forest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=500, random_state=0, max_features=32)\n",
    "classifier.fit(training, trainingSet_labels) \n",
    "prediction = classifier.predict(test)\n",
    "\n",
    "#Printing score metrics\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(testSet_labels, prediction))\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(testSet_labels, prediction))\n",
    "accuracy = accuracy_score(testSet_labels, prediction)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
